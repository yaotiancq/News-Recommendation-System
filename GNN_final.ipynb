{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de48021-09ad-49ac-aabc-0951509be4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "=== Building TRAIN graph ===\n",
      "=== Building TEST graph ===\n",
      "581\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.65 GiB. GPU 0 has a total capacity of 6.00 GiB of which 4.49 GiB is free. Of the allocated memory 594.43 MiB is allocated by PyTorch, and 7.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 330\u001b[39m\n\u001b[32m    327\u001b[39m     plt.show()\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 303\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mprint\u001b[39m(feat_dim)\n\u001b[32m    302\u001b[39m model = GNNRecommender(feat_dim, hidden_dim=\u001b[32m64\u001b[39m).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m train_model(train_data, test_data, model, epochs=\u001b[32m50\u001b[39m, lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m    305\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Final Evaluation on TEST set ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    306\u001b[39m evaluate_model(test_data, model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 220\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_data, test_data, model, epochs, lr)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    219\u001b[39m     optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     x = model(train_data)\n\u001b[32m    221\u001b[39m     logits = model.predict_logits(x, train_data.edge_index)\n\u001b[32m    222\u001b[39m     loss = loss_fn(logits, train_data.y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mGNNRecommender.<locals>.Model.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     x = torch.relu(\u001b[38;5;28mself\u001b[39m.gnn1(data.x, data.edge_index))\n\u001b[32m    202\u001b[39m     x = torch.relu(\u001b[38;5;28mself\u001b[39m.gnn2(x, data.edge_index))\n\u001b[32m    203\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.gnn3(x, data.edge_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py:134\u001b[39m, in \u001b[36mSAGEConv.forward\u001b[39m\u001b[34m(self, x, edge_index, size)\u001b[39m\n\u001b[32m    131\u001b[39m     x = (\u001b[38;5;28mself\u001b[39m.lin(x[\u001b[32m0\u001b[39m]).relu(), x[\u001b[32m1\u001b[39m])\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m out = \u001b[38;5;28mself\u001b[39m.propagate(edge_index, x=x, size=size)\n\u001b[32m    135\u001b[39m out = \u001b[38;5;28mself\u001b[39m.lin_l(out)\n\u001b[32m    137\u001b[39m x_r = x[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_j7u4y47n.py:173\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, size)\u001b[39m\n\u001b[32m    167\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.update(\n\u001b[32m    168\u001b[39m         out,\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     kwargs = \u001b[38;5;28mself\u001b[39m.collect(\n\u001b[32m    174\u001b[39m         edge_index,\n\u001b[32m    175\u001b[39m         x,\n\u001b[32m    176\u001b[39m         mutable_size,\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_j7u4y47n.py:83\u001b[39m, in \u001b[36mcollect\u001b[39m\u001b[34m(self, edge_index, x, size)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_size(size, \u001b[32m0\u001b[39m, _x_0)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     x_j = \u001b[38;5;28mself\u001b[39m._index_select(_x_0, edge_index_j)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     x_j = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:267\u001b[39m, in \u001b[36mMessagePassing._index_select\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m src.index_select(\u001b[38;5;28mself\u001b[39m.node_dim, index)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_select_safe(src, index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:290\u001b[39m, in \u001b[36mMessagePassing._index_select_safe\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index.max() >= src.size(\u001b[38;5;28mself\u001b[39m.node_dim)):\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[32m    283\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound indices in \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m\u001b[33m that are larger \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.size(\u001b[38;5;28mself\u001b[39m.node_dim)\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    287\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.size(\u001b[38;5;28mself\u001b[39m.node_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour node feature matrix and try again.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\HW2\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:271\u001b[39m, in \u001b[36mMessagePassing._index_select_safe\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_index_select_safe\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, index: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m src.index_select(\u001b[38;5;28mself\u001b[39m.node_dim, index)\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index.min() < \u001b[32m0\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 12.65 GiB. GPU 0 has a total capacity of 6.00 GiB of which 4.49 GiB is free. Of the allocated memory 594.43 MiB is allocated by PyTorch, and 7.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load entity embeddings from pretrained .vec file\n",
    "def load_entity_embedding(path):\n",
    "    embedding_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            key = parts[0]\n",
    "            vec = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
    "            embedding_dict[key] = vec\n",
    "    return embedding_dict\n",
    "\n",
    "# One-hot encode a pandas Series\n",
    "def one_hot_encode(series, mapping=None):\n",
    "    if mapping is None:\n",
    "        unique_vals = sorted(series.dropna().unique())\n",
    "        mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "    dim = len(mapping)\n",
    "    mat = np.zeros((len(series), dim), dtype=np.float32)\n",
    "    for i, val in enumerate(series):\n",
    "        if val in mapping:\n",
    "            mat[i, mapping[val]] = 1.0\n",
    "    return mat, mapping\n",
    "\n",
    "# Combine multiple content-based features into a dense vector for each news item\n",
    "def compute_news_embeddings(news_df,\n",
    "                            entity_embeddings,\n",
    "                            ent_dim,\n",
    "                            tfidf_title=None,\n",
    "                            tfidf_abs=None,\n",
    "                            cat_mapping=None,\n",
    "                            subcat_mapping=None):\n",
    "    # Encode categorical features\n",
    "    cat_vecs, cat_mapping = one_hot_encode(news_df['Category'], mapping=cat_mapping)\n",
    "    subcat_vecs, subcat_mapping = one_hot_encode(news_df['SubCategory'], mapping=subcat_mapping)\n",
    "\n",
    "    # Encode textual features using TF-IDF (title & abstract)\n",
    "    if tfidf_title is None:\n",
    "        tfidf_title = TfidfVectorizer(max_features=100)\n",
    "        title_mat = tfidf_title.fit_transform(news_df['Title'].fillna('')).toarray()\n",
    "    else:\n",
    "        title_mat = tfidf_title.transform(news_df['Title'].fillna('')).toarray()\n",
    "\n",
    "    if tfidf_abs is None:\n",
    "        tfidf_abs = TfidfVectorizer(max_features=100)\n",
    "        abs_mat = tfidf_abs.fit_transform(news_df['Abstract'].fillna('')).toarray()\n",
    "    else:\n",
    "        abs_mat = tfidf_abs.transform(news_df['Abstract'].fillna('')).toarray()\n",
    "\n",
    "    # Feature dimensions\n",
    "    cat_dim = cat_vecs.shape[1]\n",
    "    subcat_dim = subcat_vecs.shape[1]\n",
    "    title_dim = title_mat.shape[1]\n",
    "    abs_dim = abs_mat.shape[1]\n",
    "    total_dim = ent_dim + cat_dim + subcat_dim + title_dim + abs_dim\n",
    "\n",
    "    # Build embedding for each news article\n",
    "    news_features = {}\n",
    "    for idx, row in news_df.iterrows():\n",
    "        ent_ids = str(row['Entity']).split(';') if pd.notna(row['Entity']) else []\n",
    "        vecs = [entity_embeddings[e] for e in ent_ids if e in entity_embeddings]\n",
    "        ent_vec = np.mean(vecs, axis=0) if vecs else np.zeros(ent_dim, dtype=np.float32)\n",
    "\n",
    "        full = np.concatenate([\n",
    "            ent_vec,\n",
    "            cat_vecs[idx],\n",
    "            subcat_vecs[idx],\n",
    "            title_mat[idx],\n",
    "            abs_mat[idx]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        news_features[row['NewsID']] = full\n",
    "\n",
    "    return news_features, total_dim, tfidf_title, tfidf_abs, cat_mapping, subcat_mapping\n",
    "\n",
    "# Aggregate clicked news embeddings into test-time user embeddings\n",
    "def compute_user_history_embedding(behaviors_path, news_embed_path):\n",
    "    # Load precomputed news vectors\n",
    "    news_vec = {}\n",
    "    with open(news_embed_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            news_id = parts[0]\n",
    "            vec = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
    "            news_vec[news_id] = vec\n",
    "\n",
    "    # Parse click logs\n",
    "    behaviors = pd.read_csv(behaviors_path, sep='\\t', header=None,\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
    "    user_clicks = defaultdict(list)\n",
    "    for _, row in behaviors.iterrows():\n",
    "        for imp in row['Impressions'].split():\n",
    "            nid, label = imp.split('-')\n",
    "            if int(label) == 1:\n",
    "                user_clicks[row['UserID']].append(nid)\n",
    "\n",
    "    # Average clicked news vectors per user\n",
    "    user_embed = {}\n",
    "    for uid, nids in user_clicks.items():\n",
    "        vecs = [news_vec[nid] for nid in nids if nid in news_vec]\n",
    "        if vecs:\n",
    "            user_embed[uid] = np.mean(vecs, axis=0)\n",
    "    return user_embed\n",
    "\n",
    "# Build user-news bipartite graph for GNN input\n",
    "def build_graph(news_path,\n",
    "                behaviors_path,\n",
    "                entity_embedding_path,\n",
    "                tfidf_title=None,\n",
    "                tfidf_abs=None,\n",
    "                cat_mapping=None,\n",
    "                subcat_mapping=None,\n",
    "                user_history_embedding=None):\n",
    "\n",
    "    # Load raw data\n",
    "    behaviors = pd.read_csv(behaviors_path, sep='\\t', header=None,\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
    "    news = pd.read_csv(news_path, sep='\\t', header=None,\n",
    "        names=['NewsID','Category','SubCategory','Title','Abstract',\n",
    "               'URL','Entity','AbstractEntities'])\n",
    "\n",
    "    # Compute embeddings\n",
    "    entity_embeddings = load_entity_embedding(entity_embedding_path)\n",
    "    ent_dim = len(next(iter(entity_embeddings.values())))\n",
    "    news_embeds, feat_dim, tfidf_title, tfidf_abs, \\\n",
    "        cat_mapping, subcat_mapping = compute_news_embeddings(\n",
    "            news, entity_embeddings, ent_dim,\n",
    "            tfidf_title, tfidf_abs,\n",
    "            cat_mapping, subcat_mapping\n",
    "        )\n",
    "\n",
    "    # Assign unique node indices\n",
    "    user2id = {u: i for i, u in enumerate(behaviors['UserID'].unique())}\n",
    "    news2id = {n: i for i, n in enumerate(news['NewsID'].unique())}\n",
    "\n",
    "    # Create edges: user → news (offset by user count)\n",
    "    edges, labels, pairs = [], [], []\n",
    "    for _, row in behaviors.iterrows():\n",
    "        u = user2id[row['UserID']]\n",
    "        for imp in row['Impressions'].split():\n",
    "            nid, lab = imp.split('-')\n",
    "            if nid not in news2id:\n",
    "                continue\n",
    "            v = news2id[nid] + len(user2id)\n",
    "            edges.append((u, v))\n",
    "            labels.append(int(lab))\n",
    "            pairs.append((u, v))\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    y = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Node feature matrix\n",
    "    total_nodes = len(user2id) + len(news2id)\n",
    "    x = torch.zeros((total_nodes, feat_dim), dtype=torch.float32)\n",
    "\n",
    "    # Initialize user features\n",
    "    for uid, idx in user2id.items():\n",
    "        if user_history_embedding and uid in user_history_embedding:\n",
    "            x[idx] = torch.tensor(user_history_embedding[uid], dtype=torch.float32)\n",
    "        else:\n",
    "            x[idx] = torch.randn(feat_dim) * 0.01  # random init for train users\n",
    "\n",
    "    # Initialize news features\n",
    "    for nid, idx in news2id.items():\n",
    "        vec = news_embeds.get(nid, np.zeros(feat_dim, dtype=np.float32))\n",
    "        x[len(user2id) + idx] = torch.tensor(vec)\n",
    "\n",
    "    data = Data(x=x.to(device), edge_index=edge_index.to(device), y=y)\n",
    "    data.user_news_pairs = pairs\n",
    "    return data, feat_dim, tfidf_title, tfidf_abs, cat_mapping, subcat_mapping\n",
    "\n",
    "# Define 3-layer GraphSAGE model + MLP classifier\n",
    "def GNNRecommender(in_dim, hidden_dim):\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.gnn1 = SAGEConv(in_dim, hidden_dim)\n",
    "            self.gnn2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "            self.gnn3 = SAGEConv(hidden_dim, hidden_dim)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, data):\n",
    "            x = torch.relu(self.gnn1(data.x, data.edge_index))\n",
    "            x = torch.relu(self.gnn2(x, data.edge_index))\n",
    "            x = self.gnn3(x, data.edge_index)\n",
    "            return x\n",
    "\n",
    "        def predict_logits(self, x, edge_index):\n",
    "            h_u = x[edge_index[0]]\n",
    "            h_v = x[edge_index[1]]\n",
    "            return self.classifier(torch.cat([h_u, h_v], dim=1)).squeeze()\n",
    "\n",
    "    return Model().to(device)\n",
    "\n",
    "# Training loop with full-batch graph training\n",
    "def train_model(train_data, test_data, model, epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0], device=device))\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        x = model(train_data)\n",
    "        logits = model.predict_logits(x, train_data.edge_index)\n",
    "        loss = loss_fn(logits, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {ep+1}/{epochs}  Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate for overfitting\n",
    "        print(\"→ Train Eval\", end=' | ')\n",
    "        auc_t, ndcg_t = evaluate_model(train_data, model, k=5, silent=True)\n",
    "        print(\"→ Test Eval\", end=' | ')\n",
    "        auc_v, ndcg_v = evaluate_model(test_data, model, k=5, silent=True)\n",
    "\n",
    "        train_auc_list.append(auc_t)\n",
    "        test_auc_list.append(auc_v)\n",
    "        train_ndcg_list.append(ndcg_t)\n",
    "        test_ndcg_list.append(ndcg_v)\n",
    "\n",
    "# Per-user evaluation: AUC + ranking metrics\n",
    "def evaluate_model(data, model, k=5, silent=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = model(data)\n",
    "        probs = torch.sigmoid(model.predict_logits(x, data.edge_index)).cpu().numpy()\n",
    "        labels = data.y.cpu().numpy()\n",
    "        pairs = data.user_news_pairs\n",
    "\n",
    "        user_dict = defaultdict(list)\n",
    "        for i, (u, v) in enumerate(pairs):\n",
    "            user_dict[u].append((labels[i], probs[i]))\n",
    "\n",
    "        ndcg_list, prec_list, rec_list, hit_list = [], [], [], []\n",
    "        y_true_all, y_score_all = [], []\n",
    "        for u, items in user_dict.items():\n",
    "            items = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "            topk = items[:k]\n",
    "            y_true = [t for t,_ in items]\n",
    "            y_score = [s for _,s in items]\n",
    "            y_topk = [t for t,_ in topk]\n",
    "            if sum(y_true)==0: continue\n",
    "            ndcg_list.append(ndcg_score([y_true], [y_score], k=k))\n",
    "            prec_list.append(sum(y_topk)/k)\n",
    "            rec_list.append(sum(y_topk)/sum(y_true))\n",
    "            hit_list.append(1.0 if sum(y_topk)>0 else 0.0)\n",
    "            y_true_all.extend(y_true)\n",
    "            y_score_all.extend(y_score)\n",
    "\n",
    "        auc = roc_auc_score(y_true_all, y_score_all)\n",
    "        if not silent:\n",
    "            print(f\"AUC: {auc:.4f}, nDCG@{k}: {np.mean(ndcg_list):.4f}, \"\n",
    "                  f\"Precision@{k}: {np.mean(prec_list):.4f}, \"\n",
    "                  f\"Recall@{k}: {np.mean(rec_list):.4f}, \"\n",
    "                  f\"HitRate@{k}: {np.mean(hit_list):.4f}\")\n",
    "        return auc, np.mean(ndcg_list)\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    user_embed = compute_user_history_embedding(\n",
    "        'train/behaviors.tsv',\n",
    "        'train/entity_embedding.vec'\n",
    "    )\n",
    "\n",
    "    print(\"=== Building TRAIN graph ===\")\n",
    "    train_data, feat_dim, tfidf_title, tfidf_abs, cat_map, subcat_map = build_graph(\n",
    "        'train/news.tsv',\n",
    "        'train/behaviors.tsv',\n",
    "        'train/entity_embedding.vec',\n",
    "        user_history_embedding=None\n",
    "    )\n",
    "\n",
    "    print(\"=== Building TEST graph ===\")\n",
    "    test_data, _, _, _, _, _ = build_graph(\n",
    "        'test/news.tsv',\n",
    "        'test/behaviors.tsv',\n",
    "        'test/entity_embedding.vec',\n",
    "        tfidf_title=tfidf_title,\n",
    "        tfidf_abs=tfidf_abs,\n",
    "        cat_mapping=cat_map,\n",
    "        subcat_mapping=subcat_map,\n",
    "        user_history_embedding=user_embed\n",
    "    )\n",
    "    print(feat_dim)\n",
    "    model = GNNRecommender(feat_dim, hidden_dim=64).to(device)\n",
    "    train_model(train_data, test_data, model, epochs=50, lr=1e-3)\n",
    "\n",
    "    print(\"\\n=== Final Evaluation on TEST set ===\")\n",
    "    evaluate_model(test_data, model)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_auc_list, label='Train AUC')\n",
    "    plt.plot(test_auc_list, label='Test AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_ndcg_list, label='Train nDCG@5')\n",
    "    plt.plot(test_ndcg_list, label='Test nDCG@5')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('nDCG@5')\n",
    "    plt.title('nDCG@5 over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e102aa6-7334-48fd-a364-8eb286fbec00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
