{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca666df-639d-4893-a74d-74587d6b0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N19639     health       weightloss   \n",
      "2  N61837       news        newsworld   \n",
      "3  N53526     health           voices   \n",
      "4  N38324     health          medical   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1                      50 Worst Habits For Belly Fat   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
      "4  How to Get Rid of Skin Tags, According to a De...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  These seemingly harmless habits are holding yo...   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "3  I felt like I was a fraud, and being an NBA wi...   \n",
      "4  They seem harmless, but there's a very good re...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
      "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
      "\n",
      "                                              Entity  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "3  [{\"Label\": \"National Basketball Association\", ...  \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  \n",
      "Test set Basic SVD -> AUC: 0.5051, nDCG@5: 0.1387, Precision@5: 0.0463, Recall@5: 0.2037, HitRate@5: 0.1730\n",
      "Train set Basic SVD -> AUC: 0.6063, nDCG@5: 0.2799, Precision@5: 0.1009, Recall@5: 0.3850, HitRate@5: 0.4500\n",
      "Test set SVD + Category -> AUC: 0.5252, nDCG@5: 0.1463, Precision@5: 0.0493, Recall@5: 0.2164, HitRate@5: 0.1833\n",
      "Train set SVD + Category -> AUC: 0.8815, nDCG@5: 0.4884, Precision@5: 0.1729, Recall@5: 0.6171, HitRate@5: 0.7150\n",
      "Test set SVD + Category + Entity -> AUC: 0.5052, nDCG@5: 0.1434, Precision@5: 0.0485, Recall@5: 0.2126, HitRate@5: 0.1802\n",
      "Train set SVD + Category + Entity -> AUC: 0.9332, nDCG@5: 0.6028, Precision@5: 0.1988, Recall@5: 0.7031, HitRate@5: 0.7930\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "\n",
    "# load behaviors data\n",
    "def load_behaviors_sample(file_path, sample_frac=0.1, random_state=None):\n",
    "    behaviors = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                            names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
    "    behaviors_sample = behaviors.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "    return behaviors_sample\n",
    " \n",
    "# build interaction matrix\n",
    "def build_interaction_matrix(behaviors):\n",
    "    user_item_pairs = []\n",
    "    for _, row in behaviors.iterrows():\n",
    "        user = row['UserID']\n",
    "        impressions = row['Impressions'].split()\n",
    "        for imp in impressions:\n",
    "            news_id, label = imp.split('-')\n",
    "            user_item_pairs.append((user, news_id, int(label)))\n",
    "    interaction_df = pd.DataFrame(user_item_pairs, columns=['UserID', 'NewsID', 'Label'])\n",
    "    return interaction_df\n",
    "\n",
    "# load news\n",
    "def load_news(file_path):\n",
    "    news_df = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                          names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Entity', 'AbstractEntities'])\n",
    "    return news_df\n",
    "\n",
    "# category one hot encoding\n",
    "def encode_news_category(news_df):\n",
    "    categories = pd.get_dummies(news_df['Category'])\n",
    "    categories.index = news_df['NewsID']\n",
    "    news_category_map = {news_id: categories.loc[news_id].values for news_id in news_df['NewsID']}\n",
    "    category_dim = categories.shape[1]\n",
    "    return news_category_map, category_dim\n",
    "\n",
    "# load entity embedding\n",
    "def load_entity_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            key = parts[0]\n",
    "            vec = np.array([float(x) for x in parts[1:]])\n",
    "            embeddings[key] = vec\n",
    "    return embeddings\n",
    "\n",
    "# compute news entity vectors\n",
    "def compute_news_entity_vectors(news_df, entity_embeddings):\n",
    "    news_entity_vecs = {}\n",
    "    for row in news_df.itertuples():\n",
    "        entity_ids = str(row.Entity).split(';') if pd.notna(row.Entity) else []\n",
    "        vecs = [entity_embeddings[eid] for eid in entity_ids if eid in entity_embeddings]\n",
    "        if vecs:\n",
    "            mean_vec = np.mean(vecs, axis=0)\n",
    "            news_entity_vecs[row.NewsID] = mean_vec\n",
    "        else:\n",
    "            news_entity_vecs[row.NewsID] = np.zeros_like(next(iter(entity_embeddings.values())))\n",
    "    return news_entity_vecs, len(next(iter(entity_embeddings.values())))\n",
    "\n",
    "# basic SVD\n",
    "def svd_basic(interaction_df, num_factors=8, num_iter=5, lr=0.01, reg=0.5):\n",
    "    users = interaction_df['UserID'].unique()\n",
    "    items = interaction_df['NewsID'].unique()\n",
    "    user_index = {u: i for i, u in enumerate(users)}\n",
    "    item_index = {i: j for j, i in enumerate(items)}\n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "\n",
    "    P = np.random.normal(scale=1./num_factors, size=(num_users, num_factors))\n",
    "    Q = np.random.normal(scale=1./num_factors, size=(num_items, num_factors))\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        for row in interaction_df.itertuples():\n",
    "            u, i, r = user_index[row.UserID], item_index[row.NewsID], row.Label\n",
    "            pred = np.dot(P[u], Q[i])\n",
    "            err = r - pred\n",
    "            P[u] += lr * (err * Q[i] - reg * P[u])\n",
    "            Q[i] += lr * (err * P[u] - reg * Q[i])\n",
    "    return P, Q, user_index, item_index\n",
    "\n",
    "# SVD + category\n",
    "def svd_with_category(interaction_df, news_category_map, category_dim, num_factors=8, num_iter=5, lr=0.01, reg=0.5):\n",
    "    users = interaction_df['UserID'].unique()\n",
    "    items = interaction_df['NewsID'].unique()\n",
    "    user_index = {u: i for i, u in enumerate(users)}\n",
    "    item_index = {i: j for j, i in enumerate(items)}\n",
    "\n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "    total_dim = num_factors + category_dim\n",
    "\n",
    "    P = np.random.normal(scale=1./total_dim, size=(num_users, total_dim))\n",
    "    Q = np.random.normal(scale=1./num_factors, size=(num_items, num_factors))\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        for row in interaction_df.itertuples():\n",
    "            u, i, r = user_index[row.UserID], item_index[row.NewsID], row.Label\n",
    "            cat_vec = news_category_map[row.NewsID]\n",
    "            Qi_extended = np.concatenate((Q[i], cat_vec))\n",
    "            pred = np.dot(P[u], Qi_extended)\n",
    "            err = r - pred\n",
    "            P[u] += lr * (err * Qi_extended - reg * P[u])\n",
    "            Q[i] += lr * (err * P[u][:num_factors] - reg * Q[i])\n",
    "    return P, Q, user_index, item_index, news_category_map\n",
    "\n",
    "# SVD + Category + Entity\n",
    "def svd_with_category_entity(interaction_df, news_category_map, news_entity_vecs, category_dim, entity_dim,\n",
    "                              num_factors=8, num_iter=5, lr=0.01, reg=0.5):\n",
    "    users = interaction_df['UserID'].unique()\n",
    "    items = interaction_df['NewsID'].unique()\n",
    "    user_index = {u: i for i, u in enumerate(users)}\n",
    "    item_index = {i: j for j, i in enumerate(items)}\n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "    total_dim = num_factors + category_dim + entity_dim\n",
    "\n",
    "    P = np.random.normal(scale=1./total_dim, size=(num_users, total_dim))\n",
    "    Q = np.random.normal(scale=1./num_factors, size=(num_items, num_factors))\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        for row in interaction_df.itertuples():\n",
    "            u, i, r = user_index[row.UserID], item_index[row.NewsID], row.Label\n",
    "            cat_vec = news_category_map[row.NewsID]\n",
    "            entity_vec = news_entity_vecs[row.NewsID]\n",
    "            Qi_extended = np.concatenate((Q[i], cat_vec, entity_vec))\n",
    "            pred = np.dot(P[u], Qi_extended)\n",
    "            err = r - pred\n",
    "            P[u] += lr * (err * Qi_extended - reg * P[u])\n",
    "            Q[i] += lr * (err * P[u][:num_factors] - reg * Q[i])\n",
    "    return P, Q, user_index, item_index\n",
    "\n",
    "# caculate matrics\n",
    "def evaluate_sample(interaction_df, P, Q, user_index, item_index, news_category_map=None, news_entity_vecs=None, k=5):\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "    ndcg_values = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hitrate_count = 0\n",
    "\n",
    "    for user in user_index.keys():\n",
    "        u = user_index[user]\n",
    "        user_data = interaction_df[interaction_df['UserID'] == user]\n",
    "        true_labels = []\n",
    "        pred_scores = []\n",
    "        positive_news = set(user_data[user_data['Label'] == 1]['NewsID'])\n",
    "\n",
    "        for row in user_data.itertuples():\n",
    "            if row.NewsID not in item_index:\n",
    "                continue\n",
    "            i = item_index[row.NewsID]\n",
    "            if news_category_map is None:\n",
    "                Qi_ext = Q[i]\n",
    "            elif news_entity_vecs is None:\n",
    "                Qi_ext = np.concatenate((Q[i], news_category_map[row.NewsID]))\n",
    "            else:\n",
    "                Qi_ext = np.concatenate((Q[i], news_category_map[row.NewsID], news_entity_vecs[row.NewsID]))\n",
    "            pred = np.dot(P[u], Qi_ext)\n",
    "            true_labels.append(row.Label)\n",
    "            pred_scores.append(pred)\n",
    "            y_true_all.append(row.Label)\n",
    "            y_score_all.append(pred)\n",
    "\n",
    "        if len(true_labels) > 1:\n",
    "            ndcg = ndcg_score([true_labels], [pred_scores], k=k)\n",
    "            ndcg_values.append(ndcg)\n",
    "\n",
    "            sorted_indices = np.argsort(pred_scores)[::-1][:k]\n",
    "            recommended_top_k = [user_data.iloc[i].NewsID for i in sorted_indices]\n",
    "            hit_count = len(set(recommended_top_k).intersection(positive_news))\n",
    "\n",
    "            precision_list.append(hit_count / k)\n",
    "            recall_list.append(hit_count / len(positive_news) if positive_news else 0)\n",
    "            if hit_count > 0:\n",
    "                hitrate_count += 1\n",
    "\n",
    "    auc = roc_auc_score(y_true_all, y_score_all)\n",
    "    mean_ndcg = np.mean(ndcg_values)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    hitrate = hitrate_count / len(user_index)\n",
    "\n",
    "    return auc, mean_ndcg, mean_precision, mean_recall, hitrate\n",
    "\n",
    "def evaluate_on_train_sample(interaction_df_train, P, Q, user_index, item_index, news_category_map=None, news_entity_vecs=None, k=5, sample_size=1000):\n",
    "    np.random.seed(42)\n",
    "    all_users = list(user_index.keys())\n",
    "    sampled_users = np.random.choice(all_users, size=min(sample_size, len(all_users)), replace=False)\n",
    "\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "    ndcg_values = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hitrate_count = 0\n",
    "\n",
    "    for user in sampled_users:\n",
    "        u = user_index[user]\n",
    "        user_data = interaction_df_train[interaction_df_train['UserID'] == user]\n",
    "        true_labels = []\n",
    "        pred_scores = []\n",
    "        positive_news = set(user_data[user_data['Label'] == 1]['NewsID'])\n",
    "\n",
    "        for row in user_data.itertuples():\n",
    "            if row.NewsID not in item_index:\n",
    "                continue\n",
    "            i = item_index[row.NewsID]\n",
    "            if news_category_map is None:\n",
    "                Qi_ext = Q[i]\n",
    "            elif news_entity_vecs is None:\n",
    "                Qi_ext = np.concatenate((Q[i], news_category_map[row.NewsID]))\n",
    "            else:\n",
    "                Qi_ext = np.concatenate((Q[i], news_category_map[row.NewsID], news_entity_vecs[row.NewsID]))\n",
    "            pred = np.dot(P[u], Qi_ext)\n",
    "            true_labels.append(row.Label)\n",
    "            pred_scores.append(pred)\n",
    "            y_true_all.append(row.Label)\n",
    "            y_score_all.append(pred)\n",
    "\n",
    "        if len(true_labels) > 1:\n",
    "            ndcg = ndcg_score([true_labels], [pred_scores], k=k)\n",
    "            ndcg_values.append(ndcg)\n",
    "\n",
    "            sorted_indices = np.argsort(pred_scores)[::-1][:k]\n",
    "            recommended_top_k = [user_data.iloc[i].NewsID for i in sorted_indices]\n",
    "            hit_count = len(set(recommended_top_k).intersection(positive_news))\n",
    "\n",
    "            precision_list.append(hit_count / k)\n",
    "            recall_list.append(hit_count / len(positive_news) if positive_news else 0)\n",
    "            if hit_count > 0:\n",
    "                hitrate_count += 1\n",
    "\n",
    "    auc = roc_auc_score(y_true_all, y_score_all)\n",
    "    mean_ndcg = np.mean(ndcg_values)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    hitrate = hitrate_count / len(sampled_users)\n",
    "\n",
    "    return auc, mean_ndcg, mean_precision, mean_recall, hitrate\n",
    "\n",
    "# main\n",
    "# if __name__ == '__main__':\n",
    "#     behaviors = load_behaviors_sample('MINDsmall_train/behaviors.tsv', sample_frac=0.01)\n",
    "#     news_df = load_news('MINDsmall_train/news.tsv')\n",
    "#     interaction_df = build_interaction_matrix(behaviors)\n",
    "\n",
    "#     train_df = interaction_df.sample(frac=0.9, random_state=42)\n",
    "#     test_df = interaction_df.drop(train_df.index)\n",
    "\n",
    "#     news_category_map, category_dim = encode_news_category(news_df)\n",
    "#     entity_embeddings = load_entity_embeddings('MINDsmall_train/entity_embedding.vec')\n",
    "#     news_entity_vecs, entity_dim = compute_news_entity_vectors(news_df, entity_embeddings)\n",
    "\n",
    "#     # 1. Basic SVD\n",
    "#     P_svd, Q_svd, user_idx_svd, item_idx_svd = svd_basic(train_df)\n",
    "#     auc_svd, ndcg_svd, precision_svd, recall_svd, hitrate_svd = evaluate_sample(test_df_sampled, P_svd, Q_svd, user_idx_svd, item_idx_svd)\n",
    "#     print(f\"Basic SVD -> AUC: {auc_svd:.4f}, nDCG@5: {ndcg_svd:.4f}, Precision@5: {precision_svd:.4f}, Recall@5: {recall_svd:.4f}, HitRate@5: {hitrate_svd:.4f}\")\n",
    "\n",
    "#     # 2. SVD + Category\n",
    "#     P_cat, Q_cat, user_idx_cat, item_idx_cat, _ = svd_with_category(train_df, news_category_map, category_dim)\n",
    "#     auc_cat, ndcg_cat, precision_cat, recall_cat, hitrate_cat = evaluate_sample(test_df_sampled, P_cat, Q_cat, user_idx_cat, item_idx_cat, news_category_map=news_category_map)\n",
    "#     print(f\"SVD + Category -> AUC: {auc_cat:.4f}, nDCG@5: {ndcg_cat:.4f}, Precision@5: {precision_cat:.4f}, Recall@5: {recall_cat:.4f}, HitRate@5: {hitrate_cat:.4f}\")\n",
    "\n",
    "#     # 3. SVD + Category + Entity\n",
    "#     P_all, Q_all, user_idx_all, item_idx_all = svd_with_category_entity(train_df, news_category_map, news_entity_vecs, category_dim, entity_dim)\n",
    "#     auc_all, ndcg_all, precision_all, recall_all, hitrate_all = evaluate_sample(test_df_sampled, P_all, Q_all, user_idx_all, item_idx_all, news_category_map=news_category_map, news_entity_vecs=news_entity_vecs)\n",
    "#     print(f\"SVD + Category + Entity -> AUC: {auc_all:.4f}, nDCG@5: {ndcg_all:.4f}, Precision@5: {precision_all:.4f}, Recall@5: {recall_all:.4f}, HitRate@5: {hitrate_all:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    behaviors = load_behaviors_sample('MINDsmall_train/behaviors.tsv', sample_frac=0.1)\n",
    "    news_df = load_news('MINDsmall_train/news.tsv')\n",
    "    interaction_df = build_interaction_matrix(behaviors)\n",
    "\n",
    "    print(news_df.head())\n",
    "\n",
    "    train_df = interaction_df.sample(frac=0.8, random_state=42)\n",
    "    test_df = interaction_df.drop(train_df.index)\n",
    "\n",
    "    news_category_map, category_dim = encode_news_category(news_df)\n",
    "    entity_embeddings = load_entity_embeddings('MINDsmall_train/entity_embedding.vec')\n",
    "    news_entity_vecs, entity_dim = compute_news_entity_vectors(news_df, entity_embeddings)\n",
    "\n",
    "    # 1. Basic SVD\n",
    "    P_svd, Q_svd, user_idx_svd, item_idx_svd = svd_basic(train_df)\n",
    "    auc_svd, ndcg_svd, precision_svd, recall_svd, hitrate_svd = evaluate_sample(test_df, P_svd, Q_svd, user_idx_svd, item_idx_svd)\n",
    "    print(f\"Test set Basic SVD -> AUC: {auc_svd:.4f}, nDCG@5: {ndcg_svd:.4f}, Precision@5: {precision_svd:.4f}, Recall@5: {recall_svd:.4f}, HitRate@5: {hitrate_svd:.4f}\")\n",
    "\n",
    "    # Train Sample Evaluation for Basic SVD\n",
    "    auc_train_svd, ndcg_train_svd, precision_train_svd, recall_train_svd, hitrate_train_svd = evaluate_on_train_sample(\n",
    "        train_df, P_svd, Q_svd, user_idx_svd, item_idx_svd)\n",
    "    print(f\"Train set Basic SVD -> AUC: {auc_train_svd:.4f}, nDCG@5: {ndcg_train_svd:.4f}, Precision@5: {precision_train_svd:.4f}, Recall@5: {recall_train_svd:.4f}, HitRate@5: {hitrate_train_svd:.4f}\")\n",
    "\n",
    "    # 2. SVD + Category\n",
    "    P_cat, Q_cat, user_idx_cat, item_idx_cat, _ = svd_with_category(train_df, news_category_map, category_dim)\n",
    "    auc_cat, ndcg_cat, precision_cat, recall_cat, hitrate_cat = evaluate_sample(test_df, P_cat, Q_cat, user_idx_cat, item_idx_cat, news_category_map=news_category_map)\n",
    "    print(f\"Test set SVD + Category -> AUC: {auc_cat:.4f}, nDCG@5: {ndcg_cat:.4f}, Precision@5: {precision_cat:.4f}, Recall@5: {recall_cat:.4f}, HitRate@5: {hitrate_cat:.4f}\")\n",
    "\n",
    "    # Train Sample Evaluation for SVD + Category\n",
    "    auc_train_cat, ndcg_train_cat, precision_train_cat, recall_train_cat, hitrate_train_cat = evaluate_on_train_sample(\n",
    "        train_df, P_cat, Q_cat, user_idx_cat, item_idx_cat, news_category_map=news_category_map)\n",
    "    print(f\"Train set SVD + Category -> AUC: {auc_train_cat:.4f}, nDCG@5: {ndcg_train_cat:.4f}, Precision@5: {precision_train_cat:.4f}, Recall@5: {recall_train_cat:.4f}, HitRate@5: {hitrate_train_cat:.4f}\")\n",
    "\n",
    "    # 3. SVD + Category + Entity\n",
    "    P_all, Q_all, user_idx_all, item_idx_all = svd_with_category_entity(train_df, news_category_map, news_entity_vecs, category_dim, entity_dim)\n",
    "    auc_all, ndcg_all, precision_all, recall_all, hitrate_all = evaluate_sample(test_df, P_all, Q_all, user_idx_all, item_idx_all, news_category_map=news_category_map, news_entity_vecs=news_entity_vecs)\n",
    "    print(f\"Test set SVD + Category + Entity -> AUC: {auc_all:.4f}, nDCG@5: {ndcg_all:.4f}, Precision@5: {precision_all:.4f}, Recall@5: {recall_all:.4f}, HitRate@5: {hitrate_all:.4f}\")\n",
    "\n",
    "    # Train Sample Evaluation for SVD + Category + Entity\n",
    "    auc_train_all, ndcg_train_all, precision_train_all, recall_train_all, hitrate_train_all = evaluate_on_train_sample(\n",
    "        train_df, P_all, Q_all, user_idx_all, item_idx_all, news_category_map=news_category_map, news_entity_vecs=news_entity_vecs)\n",
    "    print(f\"Train set SVD + Category + Entity -> AUC: {auc_train_all:.4f}, nDCG@5: {ndcg_train_all:.4f}, Precision@5: {precision_train_all:.4f}, Recall@5: {recall_train_all:.4f}, HitRate@5: {hitrate_train_all:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a105120-ccb7-44b2-981a-0ff56d9a0d72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf269d0-7e70-4576-8481-e4998bf38660",
   "metadata": {},
   "source": [
    "# SVD-Based Recommendation Evaluation (Train vs Test)\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "| Model                      | AUC (Train) | AUC (Test) | nDCG@5 (Train) | nDCG@5 (Test) | Precision@5 (Test) | Recall@5 (Test) | HitRate@5 (Test) |\n",
    "|----------------------------|--------------|------------|----------------|---------------|---------------------|-----------------|------------------|\n",
    "| Basic SVD                  | 0.6058       | 0.5007     | 0.2735         | 0.1368        | 0.0450              | 0.2007          | 0.1712           |\n",
    "| SVD + Category             | 0.8718       | 0.5115     | 0.4670         | 0.1386        | 0.0482              | 0.2122          | 0.1806           |\n",
    "| SVD + Category + Entity    | 0.9297       | 0.4961     | 0.5785         | 0.1382        | 0.0474              | 0.2098          | 0.1783           |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- Performance on the training set is significantly higher than on the test set, indicating clear overfitting.\n",
    "- AUC on the test set is close to 0.5, meaning ranking quality is almost random on unseen data.\n",
    "- nDCG@5 and Precision@5 are low, suggesting limited practical recommendation value.\n",
    "- Adding Category and Entity features improves training performance but does not help test performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Possible Causes\n",
    "\n",
    "- Model complexity is too high due to large feature dimensions, leading to overfitting.\n",
    "- Entity features are sparse or of low quality, failing to improve generalization.\n",
    "- User-item interaction data is sparse, limiting SVD's ability to capture latent relationships.\n",
    "- Training iterations or regularization parameters are not properly tuned.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374f36f-db62-4c7f-a442-fd2efca7d6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
