{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1e352b-21ac-499e-aec2-22bd540fae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Standard User-Based ===\n",
      "Standard User-Based -> AUC: 0.5043, nDCG@5: 0.2104, Precision@5: 0.0001, Recall@5: 0.0005, HitRate@5: 0.0005\n",
      "\n",
      "=== Category-enhanced User-Based ===\n",
      "Category-enhanced User-Based -> AUC: 0.5114, nDCG@5: 0.2287, Precision@5: 0.0002, Recall@5: 0.0007, HitRate@5: 0.0009\n",
      "\n",
      "=== Category + Entity-enhanced User-Based ===\n",
      "Category + Entity-enhanced User-Based -> AUC: 0.5104, nDCG@5: 0.2284, Precision@5: 0.0003, Recall@5: 0.0011, HitRate@5: 0.0014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs_matrix\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Load behaviors.tsv with sampling\n",
    "def load_behaviors_sample(file_path, sample_frac=0.1, random_state=None):\n",
    "    behaviors = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                            names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
    "    return behaviors.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Load news.tsv\n",
    "def load_news(file_path):\n",
    "    news_df = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                          names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Entity', 'AbstractEntities'],\n",
    "                          index_col=False)\n",
    "    return news_df\n",
    "\n",
    "# Build user-news interaction DataFrame from behaviors\n",
    "def build_interaction_matrix(behaviors):\n",
    "    user_item_pairs = []\n",
    "    for _, row in behaviors.iterrows():\n",
    "        user = row['UserID']\n",
    "        impressions = row['Impressions'].split()\n",
    "        for imp in impressions:\n",
    "            news_id, label = imp.split('-')\n",
    "            user_item_pairs.append((user, news_id, int(label)))\n",
    "    return pd.DataFrame(user_item_pairs, columns=['UserID', 'NewsID', 'Label'])\n",
    "\n",
    "# One-hot encode news categories\n",
    "def encode_news_category(news_df):\n",
    "    categories = pd.get_dummies(news_df['Category'])\n",
    "    categories.index = news_df['NewsID']\n",
    "    return {news_id: categories.loc[news_id].values for news_id in news_df['NewsID']}, categories.shape[1]\n",
    "\n",
    "# Load entity embeddings from file\n",
    "def load_entity_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            embeddings[parts[0]] = np.array([float(x) for x in parts[1:]])\n",
    "    return embeddings\n",
    "\n",
    "# Compute news vectors based on entity embeddings\n",
    "def compute_news_entity_vectors(news_df, entity_embeddings):\n",
    "    news_entity_vecs = {}\n",
    "    for row in news_df.itertuples():\n",
    "        if pd.notna(row.Entity):\n",
    "            try:\n",
    "                entities = json.loads(row.Entity.replace(\"'\", '\"'))\n",
    "                wikidata_ids = [e['WikidataId'] for e in entities if 'WikidataId' in e]\n",
    "            except:\n",
    "                wikidata_ids = []\n",
    "        else:\n",
    "            wikidata_ids = []\n",
    "        vecs = [entity_embeddings[eid] for eid in wikidata_ids if eid in entity_embeddings]\n",
    "        if vecs:\n",
    "            news_entity_vecs[row.NewsID] = np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            news_entity_vecs[row.NewsID] = np.zeros_like(next(iter(entity_embeddings.values())))\n",
    "    return news_entity_vecs\n",
    "\n",
    "# Build user profile vectors based on clicked news\n",
    "def build_user_profiles(interaction_df, news_vectors):\n",
    "    user_profiles = {}\n",
    "    for user_id, group in interaction_df[interaction_df['Label'] == 1].groupby('UserID'):\n",
    "        clicked_news_ids = group['NewsID']\n",
    "        clicked_vecs = [news_vectors[nid] for nid in clicked_news_ids if nid in news_vectors]\n",
    "        if clicked_vecs:\n",
    "            user_profiles[user_id] = np.mean(clicked_vecs, axis=0)\n",
    "        else:\n",
    "            user_profiles[user_id] = np.zeros_like(next(iter(news_vectors.values())))\n",
    "    return user_profiles\n",
    "\n",
    "# Build standard binary user profiles (for standard User-Based CF)\n",
    "def build_user_profiles_standard(interaction_df):\n",
    "    users = interaction_df['UserID'].unique()\n",
    "    news = interaction_df['NewsID'].unique()\n",
    "    news_index = {n: i for i, n in enumerate(news)}\n",
    "    profiles = {}\n",
    "    for user in users:\n",
    "        vec = np.zeros(len(news))\n",
    "        for _, row in interaction_df[interaction_df['UserID'] == user].iterrows():\n",
    "            if row.Label == 1 and row.NewsID in news_index:\n",
    "                vec[news_index[row.NewsID]] = 1\n",
    "        profiles[user] = vec\n",
    "    return profiles\n",
    "\n",
    "# Compute cosine similarity between users\n",
    "def compute_user_similarity_matrix(user_profiles):\n",
    "    user_ids = list(user_profiles.keys())\n",
    "    user_vectors = np.array([user_profiles[u] for u in user_ids])\n",
    "    similarity_matrix = cs_matrix(user_vectors)\n",
    "    similarity_dict = {}\n",
    "    for i, uid in enumerate(user_ids):\n",
    "        similarity_dict[uid] = {user_ids[j]: similarity_matrix[i, j] for j in range(len(user_ids)) if i != j}\n",
    "    return similarity_dict\n",
    "\n",
    "# Build clicked news cache for each user\n",
    "def build_user_clicked_news(interaction_df):\n",
    "    return interaction_df[interaction_df['Label'] == 1].groupby('UserID')['NewsID'].apply(set).to_dict()\n",
    "\n",
    "# Recommend news for each user based on similar users\n",
    "def recommend_user_based_scored(clicked_news_cache, similarity_matrix, top_k_users=5):\n",
    "    user_recommendations = {}\n",
    "    for user in similarity_matrix:\n",
    "        similar_users = sorted(similarity_matrix[user].items(), key=lambda x: -x[1])[:top_k_users]\n",
    "        candidate_scores = {}\n",
    "        for su, sim in similar_users:\n",
    "            for news in clicked_news_cache.get(su, []):\n",
    "                if news not in clicked_news_cache.get(user, set()):  # Exclude already clicked news\n",
    "                    candidate_scores[news] = candidate_scores.get(news, 0) + sim\n",
    "        sorted_news = sorted(candidate_scores.items(), key=lambda x: -x[1])\n",
    "        user_recommendations[user] = [news for news, _ in sorted_news[:top_k_users * 5]]\n",
    "    return user_recommendations\n",
    "\n",
    "# Evaluate recommendations with multiple metrics\n",
    "def evaluate_user_based(interaction_df, user_recommendations, k=5):\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "    ndcg_values = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hitrate_count = 0\n",
    "\n",
    "    for user, rec_news in user_recommendations.items():\n",
    "        user_data = interaction_df[interaction_df['UserID'] == user]\n",
    "        true_labels, pred_scores = [], []\n",
    "        positive_news = set(user_data[user_data['Label'] == 1]['NewsID'])\n",
    "        hit_count = 0\n",
    "\n",
    "        for row in user_data.itertuples():\n",
    "            true_labels.append(row.Label)\n",
    "            pred_scores.append(1 if row.NewsID in rec_news else 0)\n",
    "            y_true_all.append(row.Label)\n",
    "            y_score_all.append(pred_scores[-1])\n",
    "\n",
    "        if len(true_labels) > 1:\n",
    "            ndcg_values.append(ndcg_score([true_labels], [pred_scores], k=k))\n",
    "\n",
    "        # Calculate precision@k, recall@k, hitrate\n",
    "        recommended_top_k = set(rec_news[:k])\n",
    "        hit_count = len(recommended_top_k.intersection(positive_news))\n",
    "        precision_list.append(hit_count / k)\n",
    "        recall_list.append(hit_count / len(positive_news) if positive_news else 0)\n",
    "        if hit_count > 0:\n",
    "            hitrate_count += 1\n",
    "\n",
    "    auc = roc_auc_score(y_true_all, y_score_all)\n",
    "    mean_ndcg = np.mean(ndcg_values)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    hitrate = hitrate_count / len(user_recommendations)\n",
    "\n",
    "    return auc, mean_ndcg, mean_precision, mean_recall, hitrate\n",
    "\n",
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    # Load and split train dataset\n",
    "    behaviors_full = load_behaviors_sample('MINDsmall_train/behaviors.tsv', sample_frac=0.05)\n",
    "    train_frac = 0.8\n",
    "    behaviors_train = behaviors_full.sample(frac=train_frac, random_state=42)\n",
    "    behaviors_test = behaviors_full.drop(behaviors_train.index).reset_index(drop=True)\n",
    "    behaviors_train = behaviors_train.reset_index(drop=True)\n",
    "\n",
    "    news_df = load_news('MINDsmall_train/news.tsv')\n",
    "\n",
    "    # Build interaction matrices\n",
    "    interaction_df_train = build_interaction_matrix(behaviors_train)\n",
    "    interaction_df_test = build_interaction_matrix(behaviors_test)\n",
    "    clicked_news_cache_train = build_user_clicked_news(interaction_df_train)\n",
    "\n",
    "    # Standard User-Based\n",
    "    print(\"\\n=== Standard User-Based ===\")\n",
    "    user_profiles_std = build_user_profiles_standard(interaction_df_train)\n",
    "    similarity_std = compute_user_similarity_matrix(user_profiles_std)\n",
    "    rec_std = recommend_user_based_scored(clicked_news_cache_train, similarity_std)\n",
    "    auc_std, ndcg_std, precision_std, recall_std, hitrate_std = evaluate_user_based(interaction_df_test, rec_std)\n",
    "    print(f\"Standard User-Based -> AUC: {auc_std:.4f}, nDCG@5: {ndcg_std:.4f}, Precision@5: {precision_std:.4f}, Recall@5: {recall_std:.4f}, HitRate@5: {hitrate_std:.4f}\")\n",
    "\n",
    "    # Category-enhanced User-Based\n",
    "    print(\"\\n=== Category-enhanced User-Based ===\")\n",
    "    news_category_map, _ = encode_news_category(news_df)\n",
    "    user_profiles_cat = build_user_profiles(interaction_df_train, news_category_map)\n",
    "    similarity_cat = compute_user_similarity_matrix(user_profiles_cat)\n",
    "    rec_cat = recommend_user_based_scored(clicked_news_cache_train, similarity_cat)\n",
    "    auc_cat, ndcg_cat, precision_cat, recall_cat, hitrate_cat = evaluate_user_based(interaction_df_test, rec_cat)\n",
    "    print(f\"Category-enhanced User-Based -> AUC: {auc_cat:.4f}, nDCG@5: {ndcg_cat:.4f}, Precision@5: {precision_cat:.4f}, Recall@5: {recall_cat:.4f}, HitRate@5: {hitrate_cat:.4f}\")\n",
    "\n",
    "    # Category + Entity-enhanced User-Based\n",
    "    print(\"\\n=== Category + Entity-enhanced User-Based ===\")\n",
    "    entity_embeddings = load_entity_embeddings('MINDsmall_train/entity_embedding.vec')\n",
    "    news_entity_vecs = compute_news_entity_vectors(news_df, entity_embeddings)\n",
    "    combined_vectors = {}\n",
    "    for news_id in news_entity_vecs:\n",
    "        combined_vectors[news_id] = np.concatenate([\n",
    "            news_entity_vecs[news_id],\n",
    "            news_category_map.get(news_id, np.zeros(len(next(iter(news_category_map.values())))))\n",
    "        ])\n",
    "    user_profiles_combined = build_user_profiles(interaction_df_train, combined_vectors)\n",
    "    similarity_combined = compute_user_similarity_matrix(user_profiles_combined)\n",
    "    rec_combined = recommend_user_based_scored(clicked_news_cache_train, similarity_combined)\n",
    "    auc_comb, ndcg_comb, precision_comb, recall_comb, hitrate_comb = evaluate_user_based(interaction_df_test, rec_combined)\n",
    "    print(f\"Category + Entity-enhanced User-Based -> AUC: {auc_comb:.4f}, nDCG@5: {ndcg_comb:.4f}, Precision@5: {precision_comb:.4f}, Recall@5: {recall_comb:.4f}, HitRate@5: {hitrate_comb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b63559-5e18-4687-a653-2f138fb705d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedRecommenderDiagnostics:\n",
    "    def __init__(self, interaction_df_train, interaction_df_test, user_recommendations, similarity_matrix):\n",
    "        self.interaction_df_train = interaction_df_train\n",
    "        self.interaction_df_test = interaction_df_test\n",
    "        self.user_recommendations = user_recommendations\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        \n",
    "        # Precompute useful sets\n",
    "        self.clicked_users_test = set(interaction_df_test[interaction_df_test['Label'] == 1]['UserID'])\n",
    "        self.recommended_users = set(user_recommendations.keys())\n",
    "        self.active_users = self.clicked_users_test.intersection(self.recommended_users)\n",
    "\n",
    "    # 1. Analyze user similarity distribution\n",
    "    def analyze_similarity_distribution(self):\n",
    "        all_sims = []\n",
    "        for user, sims in self.similarity_matrix.items():\n",
    "            all_sims.extend(list(sims.values()))\n",
    "        print(f\"Similarity count: {len(all_sims)}\")\n",
    "        print(f\"Mean similarity: {np.mean(all_sims):.4f}\")\n",
    "        print(f\"Median similarity: {np.median(all_sims):.4f}\")\n",
    "        print(f\"Similarity > 0.5: {np.sum(np.array(all_sims) > 0.5)}\")\n",
    "        print(f\"Similarity ≈ 0: {np.sum(np.array(all_sims) < 1e-3)}\")\n",
    "        return all_sims\n",
    "\n",
    "    # 2. Calculate user click overlap\n",
    "    def user_click_overlap(self):\n",
    "        user_clicks = self.interaction_df_train[self.interaction_df_train['Label'] == 1].groupby('UserID')['NewsID'].apply(set).to_dict()\n",
    "        overlaps = []\n",
    "        users = list(user_clicks.keys())\n",
    "        for i in range(len(users)):\n",
    "            for j in range(i+1, len(users)):\n",
    "                u1, u2 = users[i], users[j]\n",
    "                inter = len(user_clicks[u1].intersection(user_clicks[u2]))\n",
    "                union = len(user_clicks[u1].union(user_clicks[u2]))\n",
    "                if union > 0:\n",
    "                    overlaps.append(inter / union)\n",
    "        print(f\"Average user click overlap: {np.mean(overlaps):.4f}\")\n",
    "        return overlaps\n",
    "\n",
    "    # 3. Coverage: Are recommended news items in clicked test items?\n",
    "    def recommendation_coverage(self):\n",
    "        clicked_news_test = set(self.interaction_df_test[self.interaction_df_test['Label'] == 1]['NewsID'])\n",
    "        all_rec_news = set([news for recs in self.user_recommendations.values() for news in recs])\n",
    "        overlap_news = all_rec_news.intersection(clicked_news_test)\n",
    "        print(f\"Recommended news count: {len(all_rec_news)}\")\n",
    "        print(f\"Clicked news in test set: {len(clicked_news_test)}\")\n",
    "        print(f\"Recommended news overlapping with clicked: {len(overlap_news)}\")\n",
    "        print(f\"Overlap ratio: {len(overlap_news) / len(clicked_news_test):.2%}\")\n",
    "\n",
    "    # 4. Debug a sample of active users\n",
    "    def debug_active_users(self, num_users=5):\n",
    "        sample_users = list(self.active_users)[:num_users]\n",
    "        for user_id in sample_users:\n",
    "            user_clicks = set(self.interaction_df_test[(self.interaction_df_test['UserID'] == user_id) & (self.interaction_df_test['Label'] == 1)]['NewsID'])\n",
    "            user_recs = set(self.user_recommendations.get(user_id, []))\n",
    "            overlap = user_clicks.intersection(user_recs)\n",
    "            print(f\"User {user_id} clicked: {user_clicks}\")\n",
    "            print(f\"User {user_id} recommended: {user_recs}\")\n",
    "            print(f\"Overlap: {overlap}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    # 5. Summary of active user stats\n",
    "    def active_user_summary(self):\n",
    "        print(f\"Clicked users in test set: {len(self.clicked_users_test)}\")\n",
    "        print(f\"Users with recommendations: {len(self.recommended_users)}\")\n",
    "        print(f\"Active users (click + rec): {len(self.active_users)}\")\n",
    "\n",
    "    # 6. News overlap analysis between train and dev\n",
    "    def analyze_news_overlap(self, train_news_path, dev_news_path):\n",
    "        def load_news(file_path):\n",
    "            news_df = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                                  names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Entity'],\n",
    "                                  index_col=False)\n",
    "            return news_df\n",
    "\n",
    "        train_news = load_news(train_news_path)\n",
    "        dev_news = load_news(dev_news_path)\n",
    "\n",
    "        train_news_ids = set(train_news['NewsID'])\n",
    "        dev_news_ids = set(dev_news['NewsID'])\n",
    "\n",
    "        common_news = train_news_ids.intersection(dev_news_ids)\n",
    "        only_in_train = train_news_ids - dev_news_ids\n",
    "        only_in_dev = dev_news_ids - train_news_ids\n",
    "\n",
    "        print(f\"Train news count: {len(train_news_ids)}\")\n",
    "        print(f\"Dev news count: {len(dev_news_ids)}\")\n",
    "        print(f\"Common news count: {len(common_news)}\")\n",
    "        print(f\"News only in train: {len(only_in_train)}\")\n",
    "        print(f\"News only in dev: {len(only_in_dev)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fed6c0-2779-4313-bbfa-a23dbc997aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity count: 33125780\n",
      "Mean similarity: 0.0049\n",
      "Median similarity: 0.0000\n",
      "Similarity > 0.5: 155746\n",
      "Similarity ≈ 0: 32874700\n",
      "Average user click overlap: 0.0039\n",
      "Recommended news count: 1375\n",
      "Clicked news in test set: 1054\n",
      "Recommended news overlapping with clicked: 653\n",
      "Overlap ratio: 61.95%\n",
      "Clicked users in test set: 1530\n",
      "Users with recommendations: 5756\n",
      "Active users (click + rec): 231\n",
      "User U75298 clicked: {'N11830'}\n",
      "User U75298 recommended: {'N4642', 'N51006', 'N6767', 'N41122', 'N14029', 'N18403', 'N16419', 'N287', 'N32519', 'N23816'}\n",
      "Overlap: set()\n",
      "----------------------------------------\n",
      "User U45587 clicked: {'N47020'}\n",
      "User U45587 recommended: set()\n",
      "Overlap: set()\n",
      "----------------------------------------\n",
      "User U88642 clicked: {'N14029'}\n",
      "User U88642 recommended: set()\n",
      "Overlap: set()\n",
      "----------------------------------------\n",
      "User U25126 clicked: {'N49194', 'N9669'}\n",
      "User U25126 recommended: {'N35170', 'N21701', 'N27581', 'N32519', 'N5442', 'N23816'}\n",
      "Overlap: set()\n",
      "----------------------------------------\n",
      "User U27714 clicked: {'N31448'}\n",
      "User U27714 recommended: {'N63656', 'N14592', 'N63390', 'N57426', 'N60009', 'N31448', 'N39317', 'N20678', 'N51187'}\n",
      "Overlap: {'N31448'}\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19656\\996877370.py:71: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  news_df = pd.read_csv(file_path, sep='\\t', header=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train news count: 51282\n",
      "Dev news count: 42416\n",
      "Common news count: 28460\n",
      "News only in train: 22822\n",
      "News only in dev: 13956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19656\\996877370.py:71: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  news_df = pd.read_csv(file_path, sep='\\t', header=None,\n"
     ]
    }
   ],
   "source": [
    "# Initialize the diagnostics class\n",
    "diag = UserBasedRecommenderDiagnostics(interaction_df_train, interaction_df_test, rec_std, similarity_std)\n",
    "\n",
    "# Run analyses\n",
    "diag.analyze_similarity_distribution()\n",
    "diag.user_click_overlap()\n",
    "diag.recommendation_coverage()\n",
    "diag.active_user_summary()\n",
    "diag.debug_active_users(num_users=5)\n",
    "diag.analyze_news_overlap('MINDsmall_train/news.tsv', 'MINDsmall_dev/news.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1360bb2-8943-4601-b734-6a6015d88fc7",
   "metadata": {},
   "source": [
    "# Analysis and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3840c1-534c-47c4-bd31-84d2d55fe4e7",
   "metadata": {},
   "source": [
    "## User-Based Collaborative Filtering: Analysis and Conclusion\n",
    "\n",
    "### 1. Results Summary\n",
    "\n",
    "| Model Type                         | AUC     | nDCG@5  | Precision@5 | Recall@5 | HitRate@5 |\n",
    "|------------------------------------|---------|---------|-------------|----------|------------|\n",
    "| Standard User-Based                | 0.5043  | 0.2104  | 0.0001      | 0.0005   | 0.0005     |\n",
    "| Category-Enhanced User-Based       | 0.5114  | 0.2287  | 0.0002      | 0.0007   | 0.0009     |\n",
    "| Category + Entity-Enhanced         | 0.5104  | 0.2284  | 0.0003      | 0.0011   | 0.0014     |\n",
    "\n",
    "nDCG@5 shows some ranking improvement, but Precision@5 and Recall@5 remain close to zero, indicating limited recommendation effectiveness. AUC values remain near random.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Similarity Statistics\n",
    "\n",
    "- Total similarity pairs: 33,125,780\n",
    "- Mean similarity: 0.0049\n",
    "- Median similarity: 0.0000\n",
    "- Similarity > 0.5: 155,746 (~0.47%)\n",
    "- Similarity ≈ 0: 32,874,700 (~99.24%)\n",
    "\n",
    "Most user pairs have negligible similarity, with less than 1% showing meaningful overlap.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. User Click Overlap\n",
    "\n",
    "- Average user click overlap: 0.0039\n",
    "\n",
    "Users rarely click on the same news, limiting the effectiveness of collaborative filtering.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Recommendation Coverage\n",
    "\n",
    "- Recommended news count: 1,375\n",
    "- Clicked news in test set: 1,054\n",
    "- Recommended news overlapping clicked: 653\n",
    "- News-level overlap ratio: 61.95%\n",
    "\n",
    "While news-level coverage is reasonable, it does not translate to effective user-level recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. User Activity Breakdown\n",
    "\n",
    "- Clicked users in test set: 1,530\n",
    "- Users with recommendations: 5,756\n",
    "- Active users (click + recommendation overlap): 231\n",
    "\n",
    "Only 231 out of 1,530 clicked users had any overlap with recommended news.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Example User Cases\n",
    "\n",
    "- User U27714:\n",
    "  - Clicked: {'N31448'}\n",
    "  - Recommended: {'N31448', 'N63656', 'N14592', 'N63390', 'N57426', 'N60009', 'N39317', 'N20678', 'N51187'}\n",
    "  - Overlap: {'N31448'}\n",
    "\n",
    "- Other users: no overlap between clicked and recommended news.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- User-based collaborative filtering performs poorly due to low user similarity and sparse interaction data.\n",
    "- Category and entity feature enhancements slightly improve AUC and nDCG@5 but fail to improve top-5 precision or recall.\n",
    "- Mean similarity between users is low, with nearly all pairs having similarity near zero.\n",
    "- Despite reasonable news-level coverage, user-level hit rates are negligible.\n",
    "- User-based methods are not suitable for sparse datasets like MIND. Content-based or hybrid approaches are more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e476d-ac22-430d-8154-20dcc84bab33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
